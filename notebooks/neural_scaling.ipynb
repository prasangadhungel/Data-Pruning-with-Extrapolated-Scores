{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942eade9-8f31-4b07-98dc-2abab84628f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models as torchvision_models\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "from kmeans_pytorch import kmeans\n",
    "import wandb\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils import ResNet18, transform_train, transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b09552-eb10-4e64-a8ae-cb5c9f6b19f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torchvision_archs = sorted(name for name in torchvision_models.__dict__\n",
    "                           if name.islower() and not name.startswith(\"__\")\n",
    "                           and callable(torchvision_models.__dict__[name]))\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268651a1-2db2-4d68-9dbd-3c3515de1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnIndexDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, idx):\n",
    "        img, lab = super(ReturnIndexDataset, self).__getitem__(idx)\n",
    "        # path = super(ReturnIndexDataset, self).samples[idx]\n",
    "        return idx, img, lab, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15d61cb-8ec3-44b7-a7d7-c4fad349501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class_means(X, labels, num_clusters):\n",
    "    dim = X[0].shape[0]\n",
    "    labels_sum = {i: torch.zeros(dim) for i in range(num_clusters)}\n",
    "    labels_count = {i: 0 for i in range(num_clusters)}\n",
    "    for i in range(len(X)):\n",
    "        tensor = X[i]\n",
    "        label = int(labels[i].item())\n",
    "        labels_sum[label] += tensor\n",
    "        labels_count[label] += 1\n",
    "    labels_mean_tensor = torch.zeros((num_clusters, dim))\n",
    "    for i in range(num_clusters):\n",
    "        labels_mean_tensor[i] = labels_sum[i] / labels_count[i]\n",
    "    return labels, labels_mean_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af7906c7-183b-4e78-84b8-c05cd2566f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95287214-ff13-4209-a277-cba2508d0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_desired_samples(reps, indices, labels, base_dataset, target_dataset, cluster_centers, cluster_ids_x, quantile):\n",
    "    res_values = []\n",
    "    res_indices = []\n",
    "    res_class_labels = []\n",
    "    res_cluster_labels = []\n",
    "\n",
    "    batch_size = 16\n",
    "    num_clusters = len(cluster_centers)\n",
    "    reps_dataset = CustomDataset(reps.detach())\n",
    "    reps_dataloader = DataLoader(reps_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    indices = torch.squeeze(indices)\n",
    "    labels = torch.squeeze(labels)\n",
    "    cluster_ids_x = torch.squeeze(cluster_ids_x)\n",
    "    cluster_centers = cluster_centers.to(device)\n",
    "\n",
    "    # calculate norm\n",
    "    i = 0\n",
    "    for tensor in tqdm(reps_dataloader, desc='Calculating norms'):\n",
    "        tensor = tensor.to(device)\n",
    "        norm_tensor = torch.linalg.norm(tensor.unsqueeze(dim=1) - cluster_centers.unsqueeze(dim=0), dim=2).detach()\n",
    "        norm_tensor, norm_tensor_indecies = torch.sort(norm_tensor, dim=1)\n",
    "        res_values += (norm_tensor[:, 0] - norm_tensor[:, 1] - norm_tensor[:, 2]).tolist()\n",
    "        res_indices += (indices[batch_size * i: (i + 1) * batch_size]).tolist()\n",
    "        res_class_labels += (labels[batch_size * i: (i + 1) * batch_size]).tolist()\n",
    "        res_cluster_labels += norm_tensor_indecies[:, 0].tolist()\n",
    "        i += 1\n",
    "\n",
    "    # reordering samples and finding quantiles baesd on each class\n",
    "    cluster_scores = {k: [res_values[i] for i in range(len(res_values)) if int(res_cluster_labels[i]) == k] for k in\n",
    "                        range(len(cluster_centers))}\n",
    "\n",
    "    quantiles = {k: torch.quantile(torch.tensor(cluster_scores[k]), q=quantile) for k in\n",
    "                    range(num_clusters) if len(cluster_scores[k]) != 0}\n",
    "    score_dicts = {int(res_indices[i]): (res_values[i], int(res_class_labels[i]), int(res_cluster_labels[i])) for i\n",
    "                    in\n",
    "                    range(len(res_values))}\n",
    "    results_based_on_class = {i: [] for i in range(len(target_dataset.classes))}\n",
    "\n",
    "    # finding images which are in the quantile period\n",
    "    for k, v in tqdm(score_dicts.items(), desc='Finding images in quntile'):\n",
    "        if v[0] > quantiles[v[2]].item():\n",
    "            results_based_on_class[v[1]].append(k)\n",
    "\n",
    "    # find path of desired samples\n",
    "    img_paths = {}\n",
    "    for idx, img, label, ind in tqdm(target_dataset, desc='Gathering paths of desired samples'):\n",
    "        image_path = target_dataset.samples[idx][0]\n",
    "        if ind in results_based_on_class[label]:\n",
    "            try:\n",
    "                img_paths[label].append(image_path)\n",
    "            except KeyError:\n",
    "                img_paths[label] = [image_path]\n",
    "\n",
    "    return img_paths\n",
    "\n",
    "def reverse_normalization(images):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    un_normalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    return un_normalize(images)\n",
    "\n",
    "\n",
    "def save_outputs(dst_path, dataset, farthest_samples_paths):\n",
    "    try:\n",
    "        shutil.rmtree(dst_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    Path(dst_path).mkdir(parents=True, exist_ok=True)\n",
    "    for cls in dataset.classes:\n",
    "        Path(os.path.join(dst_path, str(cls))).mkdir(parents=True, exist_ok=True)\n",
    "    for cls, paths in farthest_samples_paths.items():\n",
    "        for i, path in enumerate(paths):\n",
    "            shutil.copy(path, os.path.join(dst_path, dataset.classes[cls]))\n",
    "\n",
    "\n",
    "def generate_representations(batch_size, model, dataloader, dataset, desc=''):\n",
    "    model.eval()\n",
    "\n",
    "    reps = torch.zeros((len(dataloader) * batch_size, 1000))\n",
    "    indices = torch.zeros((len(dataloader) * batch_size, 1))\n",
    "    labels = torch.zeros((len(dataloader) * batch_size, 1))\n",
    "    i = 0\n",
    "    for idx, tensor, label, index in tqdm(dataloader, desc=desc):\n",
    "        tensor = tensor.to(device)\n",
    "        with autocast(enabled=True):\n",
    "            feats = model(tensor)\n",
    "        reps[i * batch_size: min((i + 1) * batch_size, len(dataset))] = feats.detach().cpu()\n",
    "        labels[i * batch_size: min((i + 1) * batch_size, len(dataset))] = label[:, None]\n",
    "        indices[i * batch_size: min((i + 1) * batch_size, len(dataset))] = index[:, None]\n",
    "        i += 1\n",
    "    return reps, indices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe1890e-c883-47b1-8838-4d2b3ad7ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/swav/zipball/main\" to /home/studio-lab-user/.cache/torch/hub/main.zip\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deepcluster/swav_800ep_pretrain.pth.tar\" to /home/studio-lab-user/.cache/torch/hub/checkpoints/swav_800ep_pretrain.pth.tar\n",
      "100%|██████████| 108M/108M [00:00<00:00, 325MB/s] \n"
     ]
    }
   ],
   "source": [
    "# pretrained resnet-18 model\n",
    "model = torch.hub.load('facebookresearch/swav:main', 'resnet50', pretrained=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c811ca9-843a-43a0-a16e-45ec6806c4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 98740619.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "CIFAR10 dataset downloaded and organized successfully.\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "# Create directories for each class\n",
    "classes = cifar_dataset.classes\n",
    "data_dir = './cifar10_data'\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(data_dir, cls), exist_ok=True)\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Move images to respective class directories\n",
    "for idx, (image, label) in enumerate(cifar_dataset):\n",
    "    class_dir = os.path.join(data_dir, classes[label])\n",
    "    image_path = os.path.join(class_dir, f\"img_{idx}.jpg\")\n",
    "    tensor_image = TF.to_tensor(image)  # Convert PIL image to tensor\n",
    "    torchvision.utils.save_image(tensor_image, image_path)\n",
    "\n",
    "print(\"CIFAR10 dataset downloaded and organized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09022d1e-007a-45f3-8666-e3f22fd0c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    dataset = ReturnIndexDataset(data_path, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bce089b-cc7a-4ca2-808b-28dbb41e096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, dataset = get_data(\"./cifar10_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d81b78a-8ba9-4122-aad1-ba25fe02047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating representations:   0%|          | 0/3125 [00:00<?, ?it/s]/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "Generating representations: 100%|██████████| 3125/3125 [04:28<00:00, 11.62it/s]\n"
     ]
    }
   ],
   "source": [
    "reps, indices, labels = generate_representations(\n",
    "    batch_size=16,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    desc='Generating representations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e15df670-b186-4c42-8f36-51351b6b3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 87it [02:46,  1.92s/it, center_shift=0.000000, iteration=87, tol=0.000010] \n"
     ]
    }
   ],
   "source": [
    "data_size, dims = reps.shape\n",
    "num_clusters = len(dataset.classes)\n",
    "\n",
    "cluster_ids_x, cluster_centers = kmeans(X=reps,\n",
    "                                        num_clusters=num_clusters,\n",
    "                                        distance='euclidean',\n",
    "                                        device=device,\n",
    "                                        tol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a968d0-685d-4eae-94a7-688a82fa5fcf",
   "metadata": {},
   "source": [
    "Quantile 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d36a736-a843-4479-9e66-225e782db656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating norms: 100%|██████████| 3125/3125 [00:01<00:00, 3103.49it/s]\n",
      "Finding images in quntile: 100%|██████████| 50000/50000 [00:00<00:00, 1088643.52it/s]\n",
      "Gathering paths of desired samples: 100%|██████████| 50000/50000 [00:29<00:00, 1704.26it/s]\n"
     ]
    }
   ],
   "source": [
    "farthest_samples_paths = find_desired_samples(reps=reps,\n",
    "                                                indices=indices,\n",
    "                                                labels=labels,\n",
    "                                                base_dataset=dataset,\n",
    "                                                target_dataset=dataset,\n",
    "                                                cluster_centers=cluster_centers,\n",
    "                                                cluster_ids_x=cluster_ids_x,\n",
    "                                                quantile=0.5\n",
    "                                              )\n",
    "\n",
    "save_outputs(dst_path='./hard_samples_50/',\n",
    "                 dataset=dataset,\n",
    "                 farthest_samples_paths=farthest_samples_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574769fa-facf-4abe-b5e5-3e69fe4f3890",
   "metadata": {},
   "source": [
    "Quantile 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a56a59bb-9a22-4120-9d2b-ef8d5f081b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating norms: 100%|██████████| 3125/3125 [00:01<00:00, 2749.02it/s]\n",
      "Finding images in quntile: 100%|██████████| 50000/50000 [00:00<00:00, 859823.29it/s]\n",
      "Gathering paths of desired samples: 100%|██████████| 50000/50000 [00:24<00:00, 2034.70it/s]\n"
     ]
    }
   ],
   "source": [
    "farthest_samples_paths = find_desired_samples(reps=reps,\n",
    "                                                indices=indices,\n",
    "                                                labels=labels,\n",
    "                                                base_dataset=dataset,\n",
    "                                                target_dataset=dataset,\n",
    "                                                cluster_centers=cluster_centers,\n",
    "                                                cluster_ids_x=cluster_ids_x,\n",
    "                                                quantile=0.1\n",
    "                                              )\n",
    "\n",
    "save_outputs(dst_path='./hard_samples_10/',\n",
    "                 dataset=dataset,\n",
    "                 farthest_samples_paths=farthest_samples_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f2fe2-654b-4962-b51b-52a3eaac830e",
   "metadata": {},
   "source": [
    "Quantile 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa081f32-267b-48d5-89b1-69d7546e7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating norms: 100%|██████████| 3125/3125 [00:01<00:00, 2269.79it/s]\n",
      "Finding images in quntile: 100%|██████████| 50000/50000 [00:00<00:00, 1100058.75it/s]\n",
      "Gathering paths of desired samples: 100%|██████████| 50000/50000 [00:25<00:00, 1947.85it/s]\n"
     ]
    }
   ],
   "source": [
    "farthest_samples_paths = find_desired_samples(reps=reps,\n",
    "                                                indices=indices,\n",
    "                                                labels=labels,\n",
    "                                                base_dataset=dataset,\n",
    "                                                target_dataset=dataset,\n",
    "                                                cluster_centers=cluster_centers,\n",
    "                                                cluster_ids_x=cluster_ids_x,\n",
    "                                                quantile=0.2\n",
    "                                              )\n",
    "\n",
    "save_outputs(dst_path='./hard_samples_20/',\n",
    "                 dataset=dataset,\n",
    "                 farthest_samples_paths=farthest_samples_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55b000fa-2e2e-492c-8513-a0cfa62efc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_samples_50/airplane: 2206\n",
      "hard_samples_50/automobile: 2506\n",
      "hard_samples_50/bird: 2289\n",
      "hard_samples_50/cat: 2502\n",
      "hard_samples_50/deer: 2771\n",
      "hard_samples_50/dog: 2561\n",
      "hard_samples_50/frog: 2561\n",
      "hard_samples_50/horse: 2546\n",
      "hard_samples_50/ship: 2358\n",
      "hard_samples_50/truck: 2698\n"
     ]
    }
   ],
   "source": [
    "!find hard_samples_50 -mindepth 1 -type d -exec sh -c 'echo -n \"{}: \"; ls -1 \"{}\" | wc -l' \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29b21e-109e-43a2-b33e-f67fa69eb190",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21278d69-d770-461b-b485-16f7ff518c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c080c50b-caf0-49bc-8256-dd8af0f5ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/gr/wandb/run-20240501_131514-9fkk8fgr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prasanga/cifar10_pruning/runs/9fkk8fgr' target=\"_blank\">pruned-neural-scaling-10</a></strong> to <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prasanga/cifar10_pruning/runs/9fkk8fgr' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning/runs/9fkk8fgr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/prasanga/cifar10_pruning/runs/9fkk8fgr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f96c2fb02e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_frac = \"10\"\n",
    "wandb.init(project='cifar10_pruning', name='pruned-neural-scaling-'+prune_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca0238b-9651-4ef1-9f1f-0e21c89177c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_folder = \"./hard_samples_\"+ prune_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66126f29-7553-4b95-aa13-825bb2a1a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.555\n",
      "[epoch:1,  accuracy: 60.840\n",
      "[2,   200] loss: 0.955\n",
      "[epoch:2,  accuracy: 69.970\n",
      "[3,   200] loss: 0.720\n",
      "[epoch:3,  accuracy: 75.210\n",
      "[4,   200] loss: 0.574\n",
      "[epoch:4,  accuracy: 77.600\n",
      "[5,   200] loss: 0.458\n",
      "[epoch:5,  accuracy: 77.830\n",
      "[6,   200] loss: 0.355\n",
      "[epoch:6,  accuracy: 79.890\n",
      "[7,   200] loss: 0.267\n",
      "[epoch:7,  accuracy: 79.540\n",
      "[8,   200] loss: 0.188\n",
      "[epoch:8,  accuracy: 79.710\n",
      "[9,   200] loss: 0.125\n",
      "[epoch:9,  accuracy: 79.770\n",
      "[10,   200] loss: 0.088\n",
      "[epoch:10,  accuracy: 79.790\n",
      "[11,   200] loss: 0.079\n",
      "[epoch:11,  accuracy: 79.810\n",
      "[12,   200] loss: 0.066\n",
      "[epoch:12,  accuracy: 80.420\n",
      "[13,   200] loss: 0.051\n",
      "[epoch:13,  accuracy: 79.750\n",
      "[14,   200] loss: 0.047\n",
      "[epoch:14,  accuracy: 79.840\n",
      "[15,   200] loss: 0.041\n",
      "[epoch:15,  accuracy: 80.610\n",
      "[16,   200] loss: 0.046\n",
      "[epoch:16,  accuracy: 79.060\n",
      "[17,   200] loss: 0.051\n",
      "[epoch:17,  accuracy: 80.700\n",
      "[18,   200] loss: 0.037\n",
      "[epoch:18,  accuracy: 79.410\n",
      "[19,   200] loss: 0.030\n",
      "[epoch:19,  accuracy: 80.420\n",
      "[20,   200] loss: 0.036\n",
      "[epoch:20,  accuracy: 80.280\n",
      "[21,   200] loss: 0.030\n",
      "[epoch:21,  accuracy: 79.920\n",
      "[22,   200] loss: 0.026\n",
      "[epoch:22,  accuracy: 79.690\n",
      "[23,   200] loss: 0.030\n",
      "[epoch:23,  accuracy: 80.550\n",
      "[24,   200] loss: 0.030\n",
      "[epoch:24,  accuracy: 80.790\n",
      "[25,   200] loss: 0.025\n",
      "[epoch:25,  accuracy: 79.920\n",
      "[26,   200] loss: 0.026\n",
      "[epoch:26,  accuracy: 80.330\n",
      "[27,   200] loss: 0.020\n",
      "[epoch:27,  accuracy: 79.560\n",
      "[28,   200] loss: 0.032\n",
      "[epoch:28,  accuracy: 79.990\n",
      "[29,   200] loss: 0.015\n",
      "[epoch:29,  accuracy: 80.830\n",
      "[30,   200] loss: 0.015\n",
      "[epoch:30,  accuracy: 79.640\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # Print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2d0f0-9468-4d06-b999-bf07fdffcec3",
   "metadata": {},
   "source": [
    "Prune 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16964480-0355-45d7-a811-a107cfe6fb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9fkk8fgr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b88a867b54eedab9f7230baa3be15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▄▆▇▇██████████▇██████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>79.64</td></tr><tr><td>Final-Accuracy</td><td>79.64</td></tr><tr><td>Loss</td><td>0.01474</td></tr><tr><td>Top-5 Accuracy</td><td>98.43</td></tr><tr><td>Training Time</td><td>1134.00164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pruned-neural-scaling-10</strong> at: <a href='https://wandb.ai/prasanga/cifar10_pruning/runs/9fkk8fgr' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning/runs/9fkk8fgr</a><br/> View project at: <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240501_131514-9fkk8fgr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9fkk8fgr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83d9386cdb94cf78d20d690a02270e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112308422222364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/gr/wandb/run-20240501_133441-lvbhzpo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prasanga/cifar10_pruning/runs/lvbhzpo0' target=\"_blank\">pruned-neural-scaling-20</a></strong> to <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prasanga/cifar10_pruning/runs/lvbhzpo0' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning/runs/lvbhzpo0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.558\n",
      "[epoch:1,  accuracy: 58.700\n",
      "[2,   200] loss: 1.003\n",
      "[epoch:2,  accuracy: 68.870\n",
      "[3,   200] loss: 0.790\n",
      "[epoch:3,  accuracy: 74.590\n",
      "[4,   200] loss: 0.612\n",
      "[epoch:4,  accuracy: 76.840\n",
      "[5,   200] loss: 0.497\n",
      "[epoch:5,  accuracy: 77.390\n",
      "[6,   200] loss: 0.384\n",
      "[epoch:6,  accuracy: 78.550\n",
      "[7,   200] loss: 0.294\n",
      "[epoch:7,  accuracy: 80.140\n",
      "[8,   200] loss: 0.211\n",
      "[epoch:8,  accuracy: 79.520\n",
      "[9,   200] loss: 0.136\n",
      "[epoch:9,  accuracy: 79.120\n",
      "[10,   200] loss: 0.107\n",
      "[epoch:10,  accuracy: 79.520\n",
      "[11,   200] loss: 0.089\n",
      "[epoch:11,  accuracy: 79.580\n",
      "[12,   200] loss: 0.072\n",
      "[epoch:12,  accuracy: 79.210\n",
      "[13,   200] loss: 0.046\n",
      "[epoch:13,  accuracy: 79.530\n",
      "[14,   200] loss: 0.061\n",
      "[epoch:14,  accuracy: 80.170\n",
      "[15,   200] loss: 0.045\n",
      "[epoch:15,  accuracy: 79.740\n",
      "[16,   200] loss: 0.046\n",
      "[epoch:16,  accuracy: 79.440\n",
      "[17,   200] loss: 0.036\n",
      "[epoch:17,  accuracy: 79.370\n",
      "[18,   200] loss: 0.037\n",
      "[epoch:18,  accuracy: 80.110\n",
      "[19,   200] loss: 0.042\n",
      "[epoch:19,  accuracy: 80.140\n",
      "[20,   200] loss: 0.037\n",
      "[epoch:20,  accuracy: 78.670\n",
      "[21,   200] loss: 0.037\n",
      "[epoch:21,  accuracy: 80.120\n",
      "[22,   200] loss: 0.028\n",
      "[epoch:22,  accuracy: 80.080\n",
      "[23,   200] loss: 0.028\n",
      "[epoch:23,  accuracy: 79.670\n",
      "[24,   200] loss: 0.030\n",
      "[epoch:24,  accuracy: 80.010\n",
      "[25,   200] loss: 0.025\n",
      "[epoch:25,  accuracy: 79.680\n",
      "[26,   200] loss: 0.027\n",
      "[epoch:26,  accuracy: 80.060\n",
      "[27,   200] loss: 0.021\n",
      "[epoch:27,  accuracy: 80.100\n",
      "[28,   200] loss: 0.036\n",
      "[epoch:28,  accuracy: 79.880\n",
      "[29,   200] loss: 0.026\n",
      "[epoch:29,  accuracy: 80.180\n",
      "[30,   200] loss: 0.018\n",
      "[epoch:30,  accuracy: 79.450\n"
     ]
    }
   ],
   "source": [
    "prune_frac = \"20\"\n",
    "wandb.init(project='cifar10_pruning', name='pruned-neural-scaling-'+prune_frac)\n",
    "train_folder = \"./hard_samples_\"+ prune_frac\n",
    "\n",
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # Print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1aa3f-0526-461a-a96f-ea577b8c169e",
   "metadata": {},
   "source": [
    "Prune 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80f0d06c-18e4-4e4c-bf10-79d0c94c2d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lvbhzpo0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c729d35eeba453c879b02022b79940d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.11864264767490575, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▄▆▇▇▇████████████████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>79.45</td></tr><tr><td>Final-Accuracy</td><td>79.45</td></tr><tr><td>Loss</td><td>0.01758</td></tr><tr><td>Top-5 Accuracy</td><td>98.54</td></tr><tr><td>Training Time</td><td>1016.97523</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pruned-neural-scaling-20</strong> at: <a href='https://wandb.ai/prasanga/cifar10_pruning/runs/lvbhzpo0' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning/runs/lvbhzpo0</a><br/> View project at: <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240501_133441-lvbhzpo0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lvbhzpo0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56bb90b51bc419b9d228d3460ac33cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111226521111348, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/gr/wandb/run-20240501_135202-eer99zey</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prasanga/cifar10_pruning/runs/eer99zey' target=\"_blank\">pruned-neural-scaling-50</a></strong> to <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prasanga/cifar10_pruning' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prasanga/cifar10_pruning/runs/eer99zey' target=\"_blank\">https://wandb.ai/prasanga/cifar10_pruning/runs/eer99zey</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[epoch:1,  accuracy: 53.010\n",
      "[epoch:2,  accuracy: 60.620\n",
      "[epoch:3,  accuracy: 67.130\n",
      "[epoch:4,  accuracy: 70.040\n",
      "[epoch:5,  accuracy: 72.520\n",
      "[epoch:6,  accuracy: 73.810\n",
      "[epoch:7,  accuracy: 74.090\n",
      "[epoch:8,  accuracy: 75.330\n",
      "[epoch:9,  accuracy: 74.810\n",
      "[epoch:10,  accuracy: 75.570\n",
      "[epoch:11,  accuracy: 75.540\n",
      "[epoch:12,  accuracy: 75.870\n",
      "[epoch:13,  accuracy: 74.890\n",
      "[epoch:14,  accuracy: 75.610\n",
      "[epoch:15,  accuracy: 76.210\n",
      "[epoch:16,  accuracy: 75.050\n",
      "[epoch:17,  accuracy: 76.490\n",
      "[epoch:18,  accuracy: 75.780\n",
      "[epoch:19,  accuracy: 76.330\n",
      "[epoch:20,  accuracy: 75.430\n",
      "[epoch:21,  accuracy: 75.250\n",
      "[epoch:22,  accuracy: 76.040\n",
      "[epoch:23,  accuracy: 75.830\n",
      "[epoch:24,  accuracy: 75.370\n",
      "[epoch:25,  accuracy: 75.010\n",
      "[epoch:26,  accuracy: 76.160\n",
      "[epoch:27,  accuracy: 73.760\n",
      "[epoch:28,  accuracy: 75.870\n",
      "[epoch:29,  accuracy: 76.370\n",
      "[epoch:30,  accuracy: 74.580\n"
     ]
    }
   ],
   "source": [
    "prune_frac = \"50\"\n",
    "wandb.init(project='cifar10_pruning', name='pruned-neural-scaling-'+prune_frac)\n",
    "train_folder = \"./hard_samples_\"+ prune_frac\n",
    "\n",
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # Print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5bfc6-312c-409a-b270-ab7a74ec72f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
