{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "942eade9-8f31-4b07-98dc-2abab84628f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models as torchvision_models\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "from kmeans_pytorch import kmeans\n",
    "import wandb\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils import ResNet18, transform_train, transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b09552-eb10-4e64-a8ae-cb5c9f6b19f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torchvision_archs = sorted(name for name in torchvision_models.__dict__\n",
    "                           if name.islower() and not name.startswith(\"__\")\n",
    "                           and callable(torchvision_models.__dict__[name]))\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268651a1-2db2-4d68-9dbd-3c3515de1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnIndexDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, idx):\n",
    "        img, lab = super(ReturnIndexDataset, self).__getitem__(idx)\n",
    "        # path = super(ReturnIndexDataset, self).samples[idx]\n",
    "        return idx, img, lab, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15d61cb-8ec3-44b7-a7d7-c4fad349501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class_means(X, labels, num_clusters):\n",
    "    dim = X[0].shape[0]\n",
    "    labels_sum = {i: torch.zeros(dim) for i in range(num_clusters)}\n",
    "    labels_count = {i: 0 for i in range(num_clusters)}\n",
    "    for i in range(len(X)):\n",
    "        tensor = X[i]\n",
    "        label = int(labels[i].item())\n",
    "        labels_sum[label] += tensor\n",
    "        labels_count[label] += 1\n",
    "    labels_mean_tensor = torch.zeros((num_clusters, dim))\n",
    "    for i in range(num_clusters):\n",
    "        labels_mean_tensor[i] = labels_sum[i] / labels_count[i]\n",
    "    return labels, labels_mean_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7906c7-183b-4e78-84b8-c05cd2566f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95287214-ff13-4209-a277-cba2508d0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_desired_samples(reps, indices, labels, base_dataset, target_dataset, cluster_centers, cluster_ids_x, quantile):\n",
    "    res_values = []\n",
    "    res_indices = []\n",
    "    res_class_labels = []\n",
    "    res_cluster_labels = []\n",
    "\n",
    "    batch_size = 16\n",
    "    num_clusters = len(cluster_centers)\n",
    "    reps_dataset = CustomDataset(reps.detach())\n",
    "    reps_dataloader = DataLoader(reps_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    indices = torch.squeeze(indices)\n",
    "    labels = torch.squeeze(labels)\n",
    "    cluster_ids_x = torch.squeeze(cluster_ids_x)\n",
    "    cluster_centers = cluster_centers.to(device)\n",
    "\n",
    "    # calculate norm\n",
    "    i = 0\n",
    "    for tensor in tqdm(reps_dataloader, desc='Calculating norms'):\n",
    "        tensor = tensor.to(device)\n",
    "        norm_tensor = torch.linalg.norm(tensor.unsqueeze(dim=1) - cluster_centers.unsqueeze(dim=0), dim=2).detach()\n",
    "        norm_tensor, norm_tensor_indecies = torch.sort(norm_tensor, dim=1)\n",
    "        res_values += (-norm_tensor[:, 0]).tolist()\n",
    "        res_indices += (indices[batch_size * i: (i + 1) * batch_size]).tolist()\n",
    "        res_class_labels += (labels[batch_size * i: (i + 1) * batch_size]).tolist()\n",
    "        res_cluster_labels += norm_tensor_indecies[:, 0].tolist()\n",
    "        i += 1\n",
    "\n",
    "    # reordering samples and finding quantiles baesd on each class\n",
    "    cluster_scores = {k: [res_values[i] for i in range(len(res_values)) if int(res_cluster_labels[i]) == k] for k in\n",
    "                        range(len(cluster_centers))}\n",
    "\n",
    "    quantiles = {k: torch.quantile(torch.tensor(cluster_scores[k]), q=quantile) for k in\n",
    "                    range(num_clusters) if len(cluster_scores[k]) != 0}\n",
    "    score_dicts = {int(res_indices[i]): (res_values[i], int(res_class_labels[i]), int(res_cluster_labels[i])) for i\n",
    "                    in\n",
    "                    range(len(res_values))}\n",
    "    results_based_on_class = {i: [] for i in range(len(target_dataset.classes))}\n",
    "\n",
    "    # finding images which are in the quantile period\n",
    "    for k, v in tqdm(score_dicts.items(), desc='Finding images in quntile'):\n",
    "        if v[0] > quantiles[v[2]].item():\n",
    "            results_based_on_class[v[1]].append(k)\n",
    "\n",
    "    # find path of desired samples\n",
    "    img_paths = {}\n",
    "    for idx, img, label, ind in tqdm(target_dataset, desc='Gathering paths of desired samples'):\n",
    "        image_path = target_dataset.samples[idx][0]\n",
    "        if ind in results_based_on_class[label]:\n",
    "            try:\n",
    "                img_paths[label].append(image_path)\n",
    "            except KeyError:\n",
    "                img_paths[label] = [image_path]\n",
    "\n",
    "    return img_paths\n",
    "\n",
    "def reverse_normalization(images):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    un_normalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    return un_normalize(images)\n",
    "\n",
    "\n",
    "def save_outputs(dst_path, dataset, farthest_samples_paths):\n",
    "    try:\n",
    "        shutil.rmtree(dst_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    Path(dst_path).mkdir(parents=True, exist_ok=True)\n",
    "    for cls in dataset.classes:\n",
    "        Path(os.path.join(dst_path, str(cls))).mkdir(parents=True, exist_ok=True)\n",
    "    for cls, paths in farthest_samples_paths.items():\n",
    "        for i, path in enumerate(paths):\n",
    "            shutil.copy(path, os.path.join(dst_path, dataset.classes[cls]))\n",
    "\n",
    "\n",
    "def generate_representations(batch_size, model, dataloader, dataset, desc=''):\n",
    "    model.eval()\n",
    "\n",
    "    reps = torch.zeros((len(dataloader) * batch_size, 1000))\n",
    "    indices = torch.zeros((len(dataloader) * batch_size, 1))\n",
    "    labels = torch.zeros((len(dataloader) * batch_size, 1))\n",
    "    i = 0\n",
    "    for idx, tensor, label, index in tqdm(dataloader, desc=desc):\n",
    "        tensor = tensor.to(device)\n",
    "        with autocast(enabled=True):\n",
    "            feats = model(tensor)\n",
    "        reps[i * batch_size: min((i + 1) * batch_size, len(dataset))] = feats.detach().cpu()\n",
    "        labels[i * batch_size: min((i + 1) * batch_size, len(dataset))] = label[:, None]\n",
    "        indices[i * batch_size: min((i + 1) * batch_size, len(dataset))] = index[:, None]\n",
    "        i += 1\n",
    "    return reps, indices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe1890e-c883-47b1-8838-4d2b3ad7ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /nfs/homedirs/dhp/.cache/torch/hub/facebookresearch_swav_main\n",
      "/nfs/homedirs/dhp/anaconda3/envs/gr/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/nfs/homedirs/dhp/anaconda3/envs/gr/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# pretrained resnet-18 model\n",
    "model = torch.hub.load('facebookresearch/swav:main', 'resnet50', pretrained=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c811ca9-843a-43a0-a16e-45ec6806c4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 170498071/170498071 [00:07<00:00, 22878958.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "CIFAR10 dataset downloaded and organized successfully.\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "# Create directories for each class\n",
    "classes = cifar_dataset.classes\n",
    "data_dir = './cifar10_data'\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(data_dir, cls), exist_ok=True)\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Move images to respective class directories\n",
    "for idx, (image, label) in enumerate(cifar_dataset):\n",
    "    class_dir = os.path.join(data_dir, classes[label])\n",
    "    image_path = os.path.join(class_dir, f\"img_{idx}.jpg\")\n",
    "    tensor_image = TF.to_tensor(image)  # Convert PIL image to tensor\n",
    "    torchvision.utils.save_image(tensor_image, image_path)\n",
    "\n",
    "print(\"CIFAR10 dataset downloaded and organized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09022d1e-007a-45f3-8666-e3f22fd0c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    dataset = ReturnIndexDataset(data_path, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bce089b-cc7a-4ca2-808b-28dbb41e096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, dataset = get_data(\"./cifar10_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d81b78a-8ba9-4122-aad1-ba25fe02047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating representations: 100%|███████████████████████████████████████| 3126/3126 [00:46<00:00, 67.11it/s]\n"
     ]
    }
   ],
   "source": [
    "reps, indices, labels = generate_representations(\n",
    "    batch_size=16,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    desc='Generating representations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e15df670-b186-4c42-8f36-51351b6b3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 117it [03:36,  1.85s/it, center_shift=0.000006, iteration=117, tol=0.000010]\n"
     ]
    }
   ],
   "source": [
    "data_size, dims = reps.shape\n",
    "num_clusters = len(dataset.classes)\n",
    "\n",
    "cluster_ids_x, cluster_centers = kmeans(X=reps,\n",
    "                                        num_clusters=num_clusters,\n",
    "                                        distance='euclidean',\n",
    "                                        device=device,\n",
    "                                        tol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a968d0-685d-4eae-94a7-688a82fa5fcf",
   "metadata": {},
   "source": [
    "Quantile 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d36a736-a843-4479-9e66-225e782db656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating norms: 100%|██████████████████████████████████████████████| 3126/3126 [00:01<00:00, 1882.88it/s]\n",
      "Finding images in quntile: 100%|█████████████████████████████████| 50001/50001 [00:00<00:00, 1023091.31it/s]\n",
      "Gathering paths of desired samples: 100%|████████████████████████████| 50001/50001 [01:02<00:00, 800.57it/s]\n"
     ]
    }
   ],
   "source": [
    "farthest_samples_paths = find_desired_samples(reps=reps,\n",
    "                                                indices=indices,\n",
    "                                                labels=labels,\n",
    "                                                base_dataset=dataset,\n",
    "                                                target_dataset=dataset,\n",
    "                                                cluster_centers=cluster_centers,\n",
    "                                                cluster_ids_x=cluster_ids_x,\n",
    "                                                quantile=0.5\n",
    "                                              )\n",
    "\n",
    "save_outputs(dst_path='./easy_samples_50/',\n",
    "                 dataset=dataset,\n",
    "                 farthest_samples_paths=farthest_samples_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574769fa-facf-4abe-b5e5-3e69fe4f3890",
   "metadata": {},
   "source": [
    "Quantile 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a56a59bb-9a22-4120-9d2b-ef8d5f081b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating norms: 100%|██████████████████████████████████████████████| 3126/3126 [00:01<00:00, 1863.11it/s]\n",
      "Finding images in quntile: 100%|█████████████████████████████████| 50001/50001 [00:00<00:00, 1001841.05it/s]\n",
      "Gathering paths of desired samples: 100%|████████████████████████████| 50001/50001 [01:02<00:00, 798.45it/s]\n"
     ]
    }
   ],
   "source": [
    "farthest_samples_paths = find_desired_samples(reps=reps,\n",
    "                                                indices=indices,\n",
    "                                                labels=labels,\n",
    "                                                base_dataset=dataset,\n",
    "                                                target_dataset=dataset,\n",
    "                                                cluster_centers=cluster_centers,\n",
    "                                                cluster_ids_x=cluster_ids_x,\n",
    "                                                quantile=0.1\n",
    "                                              )\n",
    "\n",
    "save_outputs(dst_path='./easy_samples_10/',\n",
    "                 dataset=dataset,\n",
    "                 farthest_samples_paths=farthest_samples_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f2fe2-654b-4962-b51b-52a3eaac830e",
   "metadata": {},
   "source": [
    "Quantile 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa081f32-267b-48d5-89b1-69d7546e7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating norms: 100%|██████████████████████████████████████████████| 3126/3126 [00:01<00:00, 1852.45it/s]\n",
      "Finding images in quntile: 100%|█████████████████████████████████| 50001/50001 [00:00<00:00, 1007888.36it/s]\n",
      "Gathering paths of desired samples: 100%|████████████████████████████| 50001/50001 [01:02<00:00, 804.08it/s]\n"
     ]
    }
   ],
   "source": [
    "farthest_samples_paths = find_desired_samples(reps=reps,\n",
    "                                                indices=indices,\n",
    "                                                labels=labels,\n",
    "                                                base_dataset=dataset,\n",
    "                                                target_dataset=dataset,\n",
    "                                                cluster_centers=cluster_centers,\n",
    "                                                cluster_ids_x=cluster_ids_x,\n",
    "                                                quantile=0.2\n",
    "                                              )\n",
    "\n",
    "save_outputs(dst_path='./easy_samples_20/',\n",
    "                 dataset=dataset,\n",
    "                 farthest_samples_paths=farthest_samples_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55b000fa-2e2e-492c-8513-a0cfa62efc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_samples_50/ship: 2140\n",
      "easy_samples_50/cat: 2299\n",
      "easy_samples_50/truck: 2884\n",
      "easy_samples_50/horse: 2480\n",
      "easy_samples_50/airplane: 2278\n",
      "easy_samples_50/bird: 2011\n",
      "easy_samples_50/dog: 2434\n",
      "easy_samples_50/automobile: 2642\n",
      "easy_samples_50/deer: 2729\n",
      "easy_samples_50/frog: 3093\n"
     ]
    }
   ],
   "source": [
    "!find easy_samples_50 -mindepth 1 -type d -exec sh -c 'echo -n \"{}: \"; ls -1 \"{}\" | wc -l' \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29b21e-109e-43a2-b33e-f67fa69eb190",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21278d69-d770-461b-b485-16f7ff518c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c080c50b-caf0-49bc-8256-dd8af0f5ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:avy4diwn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▃▃▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>82.11</td></tr><tr><td>Final-Accuracy</td><td>82.11</td></tr><tr><td>Loss</td><td>0.18271</td></tr><tr><td>Top-5 Accuracy</td><td>98.91</td></tr><tr><td>Training Time</td><td>676.53654</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pruned-neural-scaling-50</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/avy4diwn' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/avy4diwn</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_134006-avy4diwn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:avy4diwn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_141105-epm6ic7r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/epm6ic7r' target=\"_blank\">easy-neural-scaling-10</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/epm6ic7r' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/epm6ic7r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/epm6ic7r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0d53bb27d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_frac = \"10\"\n",
    "wandb.init(project='cifar10_pruning', name='easy-neural-scaling-'+prune_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ca0238b-9651-4ef1-9f1f-0e21c89177c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_folder = \"./easy_samples_\"+ prune_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66126f29-7553-4b95-aa13-825bb2a1a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.665\n",
      "[epoch:1,  accuracy: 54.900\n",
      "[2,   200] loss: 1.100\n",
      "[epoch:2,  accuracy: 63.110\n",
      "[3,   200] loss: 0.881\n",
      "[epoch:3,  accuracy: 69.070\n",
      "[4,   200] loss: 0.747\n",
      "[epoch:4,  accuracy: 72.230\n",
      "[5,   200] loss: 0.660\n",
      "[epoch:5,  accuracy: 75.980\n",
      "[6,   200] loss: 0.601\n",
      "[epoch:6,  accuracy: 76.800\n",
      "[7,   200] loss: 0.557\n",
      "[epoch:7,  accuracy: 78.220\n",
      "[8,   200] loss: 0.512\n",
      "[epoch:8,  accuracy: 79.360\n",
      "[9,   200] loss: 0.467\n",
      "[epoch:9,  accuracy: 79.910\n",
      "[10,   200] loss: 0.444\n",
      "[epoch:10,  accuracy: 80.290\n",
      "[11,   200] loss: 0.412\n",
      "[epoch:11,  accuracy: 81.730\n",
      "[12,   200] loss: 0.382\n",
      "[epoch:12,  accuracy: 82.640\n",
      "[13,   200] loss: 0.368\n",
      "[epoch:13,  accuracy: 82.340\n",
      "[14,   200] loss: 0.339\n",
      "[epoch:14,  accuracy: 82.000\n",
      "[15,   200] loss: 0.317\n",
      "[epoch:15,  accuracy: 84.230\n",
      "[16,   200] loss: 0.295\n",
      "[epoch:16,  accuracy: 83.310\n",
      "[17,   200] loss: 0.285\n",
      "[epoch:17,  accuracy: 84.810\n",
      "[18,   200] loss: 0.262\n",
      "[epoch:18,  accuracy: 83.520\n",
      "[19,   200] loss: 0.241\n",
      "[epoch:19,  accuracy: 84.690\n",
      "[20,   200] loss: 0.230\n",
      "[epoch:20,  accuracy: 83.940\n",
      "[21,   200] loss: 0.213\n",
      "[epoch:21,  accuracy: 84.600\n",
      "[22,   200] loss: 0.205\n",
      "[epoch:22,  accuracy: 85.090\n",
      "[23,   200] loss: 0.195\n",
      "[epoch:23,  accuracy: 85.250\n",
      "[24,   200] loss: 0.176\n",
      "[epoch:24,  accuracy: 85.710\n",
      "[25,   200] loss: 0.168\n",
      "[epoch:25,  accuracy: 84.660\n",
      "[26,   200] loss: 0.149\n",
      "[epoch:26,  accuracy: 86.030\n",
      "[27,   200] loss: 0.142\n",
      "[epoch:27,  accuracy: 85.560\n",
      "[28,   200] loss: 0.139\n",
      "[epoch:28,  accuracy: 85.510\n",
      "[29,   200] loss: 0.129\n",
      "[epoch:29,  accuracy: 85.820\n",
      "[30,   200] loss: 0.116\n",
      "[epoch:30,  accuracy: 85.700\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # Print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2d0f0-9468-4d06-b999-bf07fdffcec3",
   "metadata": {},
   "source": [
    "Prune 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16964480-0355-45d7-a811-a107cfe6fb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:epm6ic7r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇▇█▇█▇████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>85.7</td></tr><tr><td>Final-Accuracy</td><td>85.7</td></tr><tr><td>Loss</td><td>0.11597</td></tr><tr><td>Top-5 Accuracy</td><td>99.27</td></tr><tr><td>Training Time</td><td>1154.72579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-neural-scaling-10</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/epm6ic7r' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/epm6ic7r</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_141105-epm6ic7r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:epm6ic7r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_143036-s98np6qp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/s98np6qp' target=\"_blank\">easy-neural-scaling-20</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/s98np6qp' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/s98np6qp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.664\n",
      "[epoch:1,  accuracy: 54.950\n",
      "[2,   200] loss: 1.111\n",
      "[epoch:2,  accuracy: 62.530\n",
      "[3,   200] loss: 0.921\n",
      "[epoch:3,  accuracy: 69.100\n",
      "[4,   200] loss: 0.776\n",
      "[epoch:4,  accuracy: 72.430\n",
      "[5,   200] loss: 0.689\n",
      "[epoch:5,  accuracy: 76.160\n",
      "[6,   200] loss: 0.613\n",
      "[epoch:6,  accuracy: 77.640\n",
      "[7,   200] loss: 0.571\n",
      "[epoch:7,  accuracy: 77.360\n",
      "[8,   200] loss: 0.527\n",
      "[epoch:8,  accuracy: 79.700\n",
      "[9,   200] loss: 0.489\n",
      "[epoch:9,  accuracy: 80.940\n",
      "[10,   200] loss: 0.457\n",
      "[epoch:10,  accuracy: 80.100\n",
      "[11,   200] loss: 0.436\n",
      "[epoch:11,  accuracy: 82.960\n",
      "[12,   200] loss: 0.407\n",
      "[epoch:12,  accuracy: 82.030\n",
      "[13,   200] loss: 0.381\n",
      "[epoch:13,  accuracy: 83.810\n",
      "[14,   200] loss: 0.358\n",
      "[epoch:14,  accuracy: 82.480\n",
      "[15,   200] loss: 0.334\n",
      "[epoch:15,  accuracy: 84.340\n",
      "[16,   200] loss: 0.315\n",
      "[epoch:16,  accuracy: 83.710\n",
      "[17,   200] loss: 0.295\n",
      "[epoch:17,  accuracy: 84.040\n",
      "[18,   200] loss: 0.285\n",
      "[epoch:18,  accuracy: 84.690\n",
      "[19,   200] loss: 0.262\n",
      "[epoch:19,  accuracy: 86.110\n",
      "[20,   200] loss: 0.240\n",
      "[epoch:20,  accuracy: 84.990\n",
      "[21,   200] loss: 0.238\n",
      "[epoch:21,  accuracy: 85.570\n",
      "[22,   200] loss: 0.210\n",
      "[epoch:22,  accuracy: 86.200\n",
      "[23,   200] loss: 0.206\n",
      "[epoch:23,  accuracy: 84.930\n",
      "[24,   200] loss: 0.189\n",
      "[epoch:24,  accuracy: 85.850\n",
      "[25,   200] loss: 0.175\n",
      "[epoch:25,  accuracy: 86.250\n",
      "[26,   200] loss: 0.178\n",
      "[epoch:26,  accuracy: 85.450\n",
      "[27,   200] loss: 0.156\n",
      "[epoch:27,  accuracy: 85.700\n",
      "[28,   200] loss: 0.151\n",
      "[epoch:28,  accuracy: 85.980\n",
      "[29,   200] loss: 0.138\n",
      "[epoch:29,  accuracy: 85.730\n",
      "[30,   200] loss: 0.130\n",
      "[epoch:30,  accuracy: 85.560\n"
     ]
    }
   ],
   "source": [
    "prune_frac = \"20\"\n",
    "wandb.init(project='cifar10_pruning', name='easy-neural-scaling-'+prune_frac)\n",
    "train_folder = \"./easy_samples_\"+ prune_frac\n",
    "\n",
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # Print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1aa3f-0526-461a-a96f-ea577b8c169e",
   "metadata": {},
   "source": [
    "Prune 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80f0d06c-18e4-4e4c-bf10-79d0c94c2d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s98np6qp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>85.56</td></tr><tr><td>Final-Accuracy</td><td>85.56</td></tr><tr><td>Loss</td><td>0.13025</td></tr><tr><td>Top-5 Accuracy</td><td>99.41</td></tr><tr><td>Training Time</td><td>1037.73031</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-neural-scaling-20</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/s98np6qp' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/s98np6qp</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_143036-s98np6qp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s98np6qp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_144810-nze54hek</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/nze54hek' target=\"_blank\">easy-neural-scaling-50</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/nze54hek' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/nze54hek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   100] loss: 1.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/homedirs/dhp/anaconda3/envs/gr/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1,  accuracy: 47.480\n",
      "[2,   100] loss: 1.242\n",
      "[epoch:2,  accuracy: 54.250\n",
      "[3,   100] loss: 1.047\n",
      "[epoch:3,  accuracy: 61.440\n",
      "[4,   100] loss: 0.916\n",
      "[epoch:4,  accuracy: 65.850\n",
      "[5,   100] loss: 0.826\n",
      "[epoch:5,  accuracy: 67.730\n",
      "[6,   100] loss: 0.740\n",
      "[epoch:6,  accuracy: 70.640\n",
      "[7,   100] loss: 0.669\n",
      "[epoch:7,  accuracy: 70.790\n",
      "[8,   100] loss: 0.630\n",
      "[epoch:8,  accuracy: 72.250\n",
      "[9,   100] loss: 0.603\n",
      "[epoch:9,  accuracy: 72.140\n",
      "[10,   100] loss: 0.550\n",
      "[epoch:10,  accuracy: 73.060\n",
      "[11,   100] loss: 0.522\n",
      "[epoch:11,  accuracy: 74.250\n",
      "[12,   100] loss: 0.507\n",
      "[epoch:12,  accuracy: 76.200\n",
      "[13,   100] loss: 0.476\n",
      "[epoch:13,  accuracy: 74.900\n",
      "[14,   100] loss: 0.440\n",
      "[epoch:14,  accuracy: 77.180\n",
      "[15,   100] loss: 0.426\n",
      "[epoch:15,  accuracy: 76.760\n",
      "[16,   100] loss: 0.405\n",
      "[epoch:16,  accuracy: 78.450\n",
      "[17,   100] loss: 0.380\n",
      "[epoch:17,  accuracy: 77.470\n",
      "[18,   100] loss: 0.363\n",
      "[epoch:18,  accuracy: 78.630\n",
      "[19,   100] loss: 0.356\n",
      "[epoch:19,  accuracy: 76.950\n",
      "[20,   100] loss: 0.331\n",
      "[epoch:20,  accuracy: 78.980\n",
      "[21,   100] loss: 0.310\n",
      "[epoch:21,  accuracy: 79.900\n",
      "[22,   100] loss: 0.288\n",
      "[epoch:22,  accuracy: 78.640\n",
      "[23,   100] loss: 0.277\n",
      "[epoch:23,  accuracy: 80.680\n",
      "[24,   100] loss: 0.265\n",
      "[epoch:24,  accuracy: 78.580\n",
      "[25,   100] loss: 0.255\n",
      "[epoch:25,  accuracy: 80.810\n",
      "[26,   100] loss: 0.241\n",
      "[epoch:26,  accuracy: 80.330\n",
      "[27,   100] loss: 0.222\n",
      "[epoch:27,  accuracy: 80.960\n",
      "[28,   100] loss: 0.209\n",
      "[epoch:28,  accuracy: 79.840\n",
      "[29,   100] loss: 0.199\n",
      "[epoch:29,  accuracy: 80.780\n",
      "[30,   100] loss: 0.194\n",
      "[epoch:30,  accuracy: 80.340\n"
     ]
    }
   ],
   "source": [
    "prune_frac = \"50\"\n",
    "wandb.init(project='cifar10_pruning', name='easy-neural-scaling-'+prune_frac)\n",
    "train_folder = \"./easy_samples_\"+ prune_frac\n",
    "\n",
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        printed = False\n",
    "        if i % 200 == 199:  # Print every 200 mini-batches\n",
    "            printed = True\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "        if len(trainloader) < 199 and not printed:\n",
    "            if i % 100 == 99:\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "                wandb.log({\"Loss\": running_loss / 100}, step=epoch * len(trainloader) + i)\n",
    "                # Log the loss to wandb, so that we can visualize it\n",
    "                running_loss = 0.0\n",
    "                step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44b5bfc6-312c-409a-b270-ab7a74ec72f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nze54hek) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▂▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇███████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>80.34</td></tr><tr><td>Final-Accuracy</td><td>80.34</td></tr><tr><td>Loss</td><td>0.19435</td></tr><tr><td>Top-5 Accuracy</td><td>98.61</td></tr><tr><td>Training Time</td><td>682.71345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-neural-scaling-50</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/nze54hek' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/nze54hek</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_144810-nze54hek/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nze54hek). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_150234-be4fjtdo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/be4fjtdo' target=\"_blank\">unpruned-again</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/be4fjtdo' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/be4fjtdo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.646\n",
      "[epoch:1,  accuracy: 59.050\n",
      "[2,   200] loss: 1.065\n",
      "[epoch:2,  accuracy: 66.510\n",
      "[3,   200] loss: 0.852\n",
      "[epoch:3,  accuracy: 69.740\n",
      "[4,   200] loss: 0.724\n",
      "[epoch:4,  accuracy: 74.000\n",
      "[5,   200] loss: 0.638\n",
      "[epoch:5,  accuracy: 78.680\n",
      "[6,   200] loss: 0.577\n",
      "[epoch:6,  accuracy: 78.700\n",
      "[7,   200] loss: 0.536\n",
      "[epoch:7,  accuracy: 80.880\n",
      "[8,   200] loss: 0.495\n",
      "[epoch:8,  accuracy: 80.900\n",
      "[9,   200] loss: 0.467\n",
      "[epoch:9,  accuracy: 81.580\n",
      "[10,   200] loss: 0.433\n",
      "[epoch:10,  accuracy: 83.060\n",
      "[11,   200] loss: 0.393\n",
      "[epoch:11,  accuracy: 83.640\n",
      "[12,   200] loss: 0.370\n",
      "[epoch:12,  accuracy: 83.610\n",
      "[13,   200] loss: 0.344\n",
      "[epoch:13,  accuracy: 84.510\n",
      "[14,   200] loss: 0.330\n",
      "[epoch:14,  accuracy: 85.040\n",
      "[15,   200] loss: 0.301\n",
      "[epoch:15,  accuracy: 84.910\n",
      "[16,   200] loss: 0.284\n",
      "[epoch:16,  accuracy: 83.940\n",
      "[17,   200] loss: 0.272\n",
      "[epoch:17,  accuracy: 85.600\n",
      "[18,   200] loss: 0.243\n",
      "[epoch:18,  accuracy: 85.220\n",
      "[19,   200] loss: 0.230\n",
      "[epoch:19,  accuracy: 86.250\n",
      "[20,   200] loss: 0.214\n",
      "[epoch:20,  accuracy: 85.650\n",
      "[21,   200] loss: 0.199\n",
      "[epoch:21,  accuracy: 85.910\n",
      "[22,   200] loss: 0.191\n",
      "[epoch:22,  accuracy: 86.210\n",
      "[23,   200] loss: 0.176\n",
      "[epoch:23,  accuracy: 86.910\n",
      "[24,   200] loss: 0.171\n",
      "[epoch:24,  accuracy: 86.910\n",
      "[25,   200] loss: 0.154\n",
      "[epoch:25,  accuracy: 86.590\n",
      "[26,   200] loss: 0.142\n",
      "[epoch:26,  accuracy: 86.310\n",
      "[27,   200] loss: 0.137\n",
      "[epoch:27,  accuracy: 87.070\n",
      "[28,   200] loss: 0.124\n",
      "[epoch:28,  accuracy: 87.630\n",
      "[29,   200] loss: 0.119\n",
      "[epoch:29,  accuracy: 86.890\n",
      "[30,   200] loss: 0.105\n",
      "[epoch:30,  accuracy: 87.290\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='cifar10_pruning', name='unpruned-again')\n",
    "train_folder = \"./cifar10_data\"\n",
    "\n",
    "trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the ConvNet model\n",
    "net = ResNet18().to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Train the ConvNet\n",
    "start_time = time.time()\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        printed = False\n",
    "        if i % 200 == 199:  # Print every 200 mini-batches\n",
    "            printed = True\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 200))\n",
    "            wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "            # Log the loss to wandb, so that we can visualize it\n",
    "            running_loss = 0.0\n",
    "            step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "        if len(trainloader) < 199 and not printed:\n",
    "            if i % 100 == 99:\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "                wandb.log({\"Loss\": running_loss / 100}, step=epoch * len(trainloader) + i)\n",
    "                # Log the loss to wandb, so that we can visualize it\n",
    "                running_loss = 0.0\n",
    "                step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('[epoch:%d,  accuracy: %.3f' % (epoch + 1, accuracy))\n",
    "    wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Test the ConvNet on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "top5_accuracy = 100 * top5_correct / total\n",
    "wandb.log({\"Final-Accuracy\": accuracy, \"Top-5 Accuracy\": top5_accuracy, \"Training Time\": training_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34adf978-2210-47c3-af11-4e83d1d3eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2b23d59-b884-411b-a2ed-540d83bb5385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vzgy7m7n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">random-prune-again-10</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/vzgy7m7n' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/vzgy7m7n</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_152405-vzgy7m7n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vzgy7m7n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_152536-31qwd2a1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/31qwd2a1' target=\"_blank\">random-prune-again-10</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/31qwd2a1' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/31qwd2a1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.700\n",
      "[epoch:1,  accuracy: 56.370\n",
      "[2,   200] loss: 1.142\n",
      "[epoch:2,  accuracy: 64.340\n",
      "[3,   200] loss: 0.937\n",
      "[epoch:3,  accuracy: 70.450\n",
      "[4,   200] loss: 0.790\n",
      "[epoch:4,  accuracy: 74.980\n",
      "[5,   200] loss: 0.701\n",
      "[epoch:5,  accuracy: 75.830\n",
      "[6,   200] loss: 0.628\n",
      "[epoch:6,  accuracy: 78.930\n",
      "[7,   200] loss: 0.572\n",
      "[epoch:7,  accuracy: 80.300\n",
      "[8,   200] loss: 0.533\n",
      "[epoch:8,  accuracy: 80.120\n",
      "[9,   200] loss: 0.493\n",
      "[epoch:9,  accuracy: 81.420\n",
      "[10,   200] loss: 0.461\n",
      "[epoch:10,  accuracy: 82.420\n",
      "[11,   200] loss: 0.428\n",
      "[epoch:11,  accuracy: 82.890\n",
      "[12,   200] loss: 0.396\n",
      "[epoch:12,  accuracy: 83.410\n",
      "[13,   200] loss: 0.382\n",
      "[epoch:13,  accuracy: 82.970\n",
      "[14,   200] loss: 0.360\n",
      "[epoch:14,  accuracy: 83.860\n",
      "[15,   200] loss: 0.328\n",
      "[epoch:15,  accuracy: 84.690\n",
      "[16,   200] loss: 0.314\n",
      "[epoch:16,  accuracy: 85.450\n",
      "[17,   200] loss: 0.298\n",
      "[epoch:17,  accuracy: 85.630\n",
      "[18,   200] loss: 0.275\n",
      "[epoch:18,  accuracy: 86.080\n",
      "[19,   200] loss: 0.256\n",
      "[epoch:19,  accuracy: 85.520\n",
      "[20,   200] loss: 0.244\n",
      "[epoch:20,  accuracy: 86.300\n",
      "[21,   200] loss: 0.223\n",
      "[epoch:21,  accuracy: 86.070\n",
      "[22,   200] loss: 0.209\n",
      "[epoch:22,  accuracy: 85.600\n",
      "[23,   200] loss: 0.195\n",
      "[epoch:23,  accuracy: 86.590\n",
      "[24,   200] loss: 0.180\n",
      "[epoch:24,  accuracy: 86.650\n",
      "[25,   200] loss: 0.168\n",
      "[epoch:25,  accuracy: 87.250\n",
      "[26,   200] loss: 0.167\n",
      "[epoch:26,  accuracy: 87.040\n",
      "[27,   200] loss: 0.153\n",
      "[epoch:27,  accuracy: 86.710\n",
      "[28,   200] loss: 0.142\n",
      "[epoch:28,  accuracy: 87.130\n",
      "[29,   200] loss: 0.130\n",
      "[epoch:29,  accuracy: 87.150\n",
      "[30,   200] loss: 0.121\n",
      "[epoch:30,  accuracy: 86.970\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>86.97</td></tr><tr><td>Final-Accuracy</td><td>86.97</td></tr><tr><td>Loss</td><td>0.12119</td></tr><tr><td>Top-5 Accuracy</td><td>99.55</td></tr><tr><td>Training Time</td><td>1164.85288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">random-prune-again-10</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/31qwd2a1' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/31qwd2a1</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_152536-31qwd2a1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_154520-0jc50bul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/0jc50bul' target=\"_blank\">random-prune-again-20</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/0jc50bul' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/0jc50bul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   200] loss: 1.656\n",
      "[epoch:1,  accuracy: 56.670\n",
      "[2,   200] loss: 1.169\n",
      "[epoch:2,  accuracy: 62.050\n",
      "[3,   200] loss: 0.952\n",
      "[epoch:3,  accuracy: 68.670\n",
      "[4,   200] loss: 0.806\n",
      "[epoch:4,  accuracy: 72.860\n",
      "[5,   200] loss: 0.710\n",
      "[epoch:5,  accuracy: 75.110\n",
      "[6,   200] loss: 0.643\n",
      "[epoch:6,  accuracy: 78.590\n",
      "[7,   200] loss: 0.597\n",
      "[epoch:7,  accuracy: 76.840\n",
      "[8,   200] loss: 0.550\n",
      "[epoch:8,  accuracy: 79.680\n",
      "[9,   200] loss: 0.505\n",
      "[epoch:9,  accuracy: 78.730\n",
      "[10,   200] loss: 0.484\n",
      "[epoch:10,  accuracy: 81.340\n",
      "[11,   200] loss: 0.443\n",
      "[epoch:11,  accuracy: 81.000\n",
      "[12,   200] loss: 0.430\n",
      "[epoch:12,  accuracy: 81.920\n",
      "[13,   200] loss: 0.405\n",
      "[epoch:13,  accuracy: 82.820\n",
      "[14,   200] loss: 0.376\n",
      "[epoch:14,  accuracy: 84.140\n",
      "[15,   200] loss: 0.348\n",
      "[epoch:15,  accuracy: 83.760\n",
      "[16,   200] loss: 0.324\n",
      "[epoch:16,  accuracy: 84.400\n",
      "[17,   200] loss: 0.306\n",
      "[epoch:17,  accuracy: 84.490\n",
      "[18,   200] loss: 0.293\n",
      "[epoch:18,  accuracy: 84.240\n",
      "[19,   200] loss: 0.269\n",
      "[epoch:19,  accuracy: 85.230\n",
      "[20,   200] loss: 0.255\n",
      "[epoch:20,  accuracy: 84.940\n",
      "[21,   200] loss: 0.239\n",
      "[epoch:21,  accuracy: 84.740\n",
      "[22,   200] loss: 0.219\n",
      "[epoch:22,  accuracy: 84.380\n",
      "[23,   200] loss: 0.207\n",
      "[epoch:23,  accuracy: 85.000\n",
      "[24,   200] loss: 0.191\n",
      "[epoch:24,  accuracy: 85.030\n",
      "[25,   200] loss: 0.183\n",
      "[epoch:25,  accuracy: 85.420\n",
      "[26,   200] loss: 0.173\n",
      "[epoch:26,  accuracy: 84.440\n",
      "[27,   200] loss: 0.162\n",
      "[epoch:27,  accuracy: 85.990\n",
      "[28,   200] loss: 0.141\n",
      "[epoch:28,  accuracy: 85.660\n",
      "[29,   200] loss: 0.140\n",
      "[epoch:29,  accuracy: 85.900\n",
      "[30,   200] loss: 0.136\n",
      "[epoch:30,  accuracy: 85.890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▂▄▅▅▆▆▆▆▇▇▇▇█▇███████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>85.89</td></tr><tr><td>Final-Accuracy</td><td>85.89</td></tr><tr><td>Loss</td><td>0.13626</td></tr><tr><td>Top-5 Accuracy</td><td>99.22</td></tr><tr><td>Training Time</td><td>1035.78863</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">random-prune-again-20</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/0jc50bul' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/0jc50bul</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_154520-0jc50bul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/homedirs/dhp/unsupervised-data-pruning/notebooks/wandb/run-20240505_160249-ty9salup</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/ty9salup' target=\"_blank\">random-prune-again-50</a></strong> to <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/ty9salup' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/ty9salup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,   100] loss: 1.828\n",
      "[epoch:1,  accuracy: 48.110\n",
      "[2,   100] loss: 1.353\n",
      "[epoch:2,  accuracy: 56.050\n",
      "[3,   100] loss: 1.148\n",
      "[epoch:3,  accuracy: 59.720\n",
      "[4,   100] loss: 1.023\n",
      "[epoch:4,  accuracy: 66.470\n",
      "[5,   100] loss: 0.890\n",
      "[epoch:5,  accuracy: 69.280\n",
      "[6,   100] loss: 0.805\n",
      "[epoch:6,  accuracy: 73.170\n",
      "[7,   100] loss: 0.724\n",
      "[epoch:7,  accuracy: 74.730\n",
      "[8,   100] loss: 0.671\n",
      "[epoch:8,  accuracy: 75.560\n",
      "[9,   100] loss: 0.610\n",
      "[epoch:9,  accuracy: 75.190\n",
      "[10,   100] loss: 0.590\n",
      "[epoch:10,  accuracy: 77.540\n",
      "[11,   100] loss: 0.541\n",
      "[epoch:11,  accuracy: 77.190\n",
      "[12,   100] loss: 0.516\n",
      "[epoch:12,  accuracy: 78.680\n",
      "[13,   100] loss: 0.501\n",
      "[epoch:13,  accuracy: 79.510\n",
      "[14,   100] loss: 0.461\n",
      "[epoch:14,  accuracy: 80.490\n",
      "[15,   100] loss: 0.427\n",
      "[epoch:15,  accuracy: 80.720\n",
      "[16,   100] loss: 0.409\n",
      "[epoch:16,  accuracy: 80.170\n",
      "[17,   100] loss: 0.386\n",
      "[epoch:17,  accuracy: 81.460\n",
      "[18,   100] loss: 0.372\n",
      "[epoch:18,  accuracy: 81.140\n",
      "[19,   100] loss: 0.342\n",
      "[epoch:19,  accuracy: 81.410\n",
      "[20,   100] loss: 0.328\n",
      "[epoch:20,  accuracy: 82.030\n",
      "[21,   100] loss: 0.312\n",
      "[epoch:21,  accuracy: 81.510\n",
      "[22,   100] loss: 0.290\n",
      "[epoch:22,  accuracy: 81.500\n",
      "[23,   100] loss: 0.271\n",
      "[epoch:23,  accuracy: 82.150\n",
      "[24,   100] loss: 0.252\n",
      "[epoch:24,  accuracy: 83.180\n",
      "[25,   100] loss: 0.258\n",
      "[epoch:25,  accuracy: 82.240\n",
      "[26,   100] loss: 0.230\n",
      "[epoch:26,  accuracy: 81.760\n",
      "[27,   100] loss: 0.210\n",
      "[epoch:27,  accuracy: 82.970\n",
      "[28,   100] loss: 0.197\n",
      "[epoch:28,  accuracy: 83.490\n",
      "[29,   100] loss: 0.187\n",
      "[epoch:29,  accuracy: 82.660\n",
      "[30,   100] loss: 0.185\n",
      "[epoch:30,  accuracy: 82.140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▃▃▅▅▆▆▆▆▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Final-Accuracy</td><td>▁</td></tr><tr><td>Loss</td><td>█▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Top-5 Accuracy</td><td>▁</td></tr><tr><td>Training Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>82.14</td></tr><tr><td>Final-Accuracy</td><td>82.14</td></tr><tr><td>Loss</td><td>0.18526</td></tr><tr><td>Top-5 Accuracy</td><td>98.92</td></tr><tr><td>Training Time</td><td>683.03586</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">random-prune-again-50</strong> at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/ty9salup' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning/runs/ty9salup</a><br/> View project at: <a href='https://wandb.ai/unsupervised-data-pruning/cifar10_pruning' target=\"_blank\">https://wandb.ai/unsupervised-data-pruning/cifar10_pruning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240505_160249-ty9salup/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for prune_percentage in [0.1, 0.2, 0.5]:\n",
    "    str_prune_percentage = str(int(prune_percentage * 100))\n",
    "    wandb.init(project=\"cifar10_pruning\", name=\"random-prune-again-\" + str_prune_percentage)\n",
    "\n",
    "    trainset = datasets.ImageFolder(train_folder, transform=transform_train)\n",
    "\n",
    "    num_samples = len(trainset)\n",
    "    frac_to_keep = 1 - prune_percentage\n",
    "    num_samples_to_keep = int(frac_to_keep * num_samples)\n",
    "\n",
    "    # Generate a random list of indices to keep\n",
    "    indices_to_keep = random.sample(range(num_samples), num_samples_to_keep)\n",
    "\n",
    "    pruned_trainset = torch.utils.data.Subset(trainset, indices_to_keep)\n",
    "\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        pruned_trainset, batch_size=128, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=100, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize the ConvNet model\n",
    "    net = ResNet18().to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    # Train the ConvNet\n",
    "    start_time = time.time()\n",
    "    for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            printed = False\n",
    "            if i % 200 == 199:  # Print every 200 mini-batches\n",
    "                printed = True\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 200))\n",
    "                wandb.log({\"Loss\": running_loss / 200}, step=epoch * len(trainloader) + i)\n",
    "                # Log the loss to wandb, so that we can visualize it\n",
    "                running_loss = 0.0\n",
    "                step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "            if len(trainloader) < 199 and not printed:\n",
    "                if i % 100 == 99:\n",
    "                    print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 100))\n",
    "                    wandb.log({\"Loss\": running_loss / 100}, step=epoch * len(trainloader) + i)\n",
    "                    # Log the loss to wandb, so that we can visualize it\n",
    "                    running_loss = 0.0\n",
    "                    step_val = epoch * len(trainloader) + i + 1\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(\"[epoch:%d,  accuracy: %.3f\" % (epoch + 1, accuracy))\n",
    "        wandb.log({\"Accuracy\": accuracy}, step=step_val)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Test the ConvNet on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    top5_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, top5_predicted = torch.topk(outputs.data, 5, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            top5_correct += (top5_predicted == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    top5_accuracy = 100 * top5_correct / total\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"Final-Accuracy\": accuracy,\n",
    "            \"Top-5 Accuracy\": top5_accuracy,\n",
    "            \"Training Time\": training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34be1a-9633-4d7c-9561-9d384dd91cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
